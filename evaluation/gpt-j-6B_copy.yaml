alg_name: MEMIT
attn_module_tmp: transformer.h.{}.attn
clamp_norm_factor: 0.75
device: 0,1,2
fact_token: subject_last
kl_factor: 0.0625
layer_module_tmp: transformer.h.{}
layer_selection: all
layers:
- 4
- 5
- 6
- 7
- 8
lm_head_module: lm_head
ln_f_module: transformer.ln_f
mlp_module_tmp: transformer.h.{}.mlp
model_name: /home/liujiaxiang/xingboxuan/gpt-j-6b
model_parallel: true
mom2_adjustment: true
mom2_dataset: wikipedia
mom2_dtype: float32
mom2_n_samples: 100000
mom2_update_weight: 15000
rewrite_module_tmp: transformer.h.{}.mlp.fc_out
stats_dir: /home/liujiaxiang/xingboxuan/STATS_DIR
v_loss_layer: 27
v_lr: 5e-1
v_num_grad_steps: 25
v_weight_decay: 0.5
