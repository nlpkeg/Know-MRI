{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684115e6-e510-45de-ac97-5f87cb3b1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'kv_template' has no attribute 'dataset_info'\n",
      "Loaded dataset with 1209 elements\n",
      "Apply mapping = False\n",
      "loading model: /root/autodl-fs/internlm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004957914352416992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 8,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d21beb5f71f4e919f157b655e18645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-fs/internlm were not used when initializing InternLMForCausalLM: ['model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n",
      "- This IS expected if you are initializing InternLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing InternLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-fs/internlm model loaded\n",
      "{'output': 'Antarctica', 'origin_data': [{'Layer name': 'Layer_0', 'predicted_token': ' course'}, {'Layer name': 'Layer_1', 'predicted_token': ' Hon'}, {'Layer name': 'Layer_2', 'predicted_token': 'ania'}, {'Layer name': 'Layer_3', 'predicted_token': ' Europe'}, {'Layer name': 'Layer_4', 'predicted_token': 'ium'}, {'Layer name': 'Layer_5', 'predicted_token': 'rine'}, {'Layer name': 'Layer_6', 'predicted_token': ' Europe'}, {'Layer name': 'Layer_7', 'predicted_token': ' Europe'}, {'Layer name': 'Layer_8', 'predicted_token': ' Europe'}, {'Layer name': 'Layer_9', 'predicted_token': ' unknown'}, {'Layer name': 'Layer_10', 'predicted_token': ' unknown'}, {'Layer name': 'Layer_11', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_12', 'predicted_token': ' unknown'}, {'Layer name': 'Layer_13', 'predicted_token': ' unint'}, {'Layer name': 'Layer_14', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_15', 'predicted_token': ' unint'}, {'Layer name': 'Layer_16', 'predicted_token': ' unc'}, {'Layer name': 'Layer_17', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_18', 'predicted_token': ' Unc'}, {'Layer name': 'Layer_19', 'predicted_token': ' Tran'}, {'Layer name': 'Layer_20', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_21', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_22', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_23', 'predicted_token': ' Africa'}, {'Layer name': 'Layer_24', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_25', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_26', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_27', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_28', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_29', 'predicted_token': ' Antar'}, {'Layer name': 'Layer_30', 'predicted_token': ' Antar'}], 'table': [{'table_name': 'Predicted Next Tokens by Layer Source', 'table_des': 'This table shows the predicted next tokens for each layer source.', 'table_list': [{'Layer name': 'Layer_0', 'Next Token Predicted by Model': ' course'}, {'Layer name': 'Layer_1', 'Next Token Predicted by Model': ' Hon'}, {'Layer name': 'Layer_2', 'Next Token Predicted by Model': 'ania'}, {'Layer name': 'Layer_3', 'Next Token Predicted by Model': ' Europe'}, {'Layer name': 'Layer_4', 'Next Token Predicted by Model': 'ium'}, {'Layer name': 'Layer_5', 'Next Token Predicted by Model': 'rine'}, {'Layer name': 'Layer_6', 'Next Token Predicted by Model': ' Europe'}, {'Layer name': 'Layer_7', 'Next Token Predicted by Model': ' Europe'}, {'Layer name': 'Layer_8', 'Next Token Predicted by Model': ' Europe'}, {'Layer name': 'Layer_9', 'Next Token Predicted by Model': ' unknown'}, {'Layer name': 'Layer_10', 'Next Token Predicted by Model': ' unknown'}, {'Layer name': 'Layer_11', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_12', 'Next Token Predicted by Model': ' unknown'}, {'Layer name': 'Layer_13', 'Next Token Predicted by Model': ' unint'}, {'Layer name': 'Layer_14', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_15', 'Next Token Predicted by Model': ' unint'}, {'Layer name': 'Layer_16', 'Next Token Predicted by Model': ' unc'}, {'Layer name': 'Layer_17', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_18', 'Next Token Predicted by Model': ' Unc'}, {'Layer name': 'Layer_19', 'Next Token Predicted by Model': ' Tran'}, {'Layer name': 'Layer_20', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_21', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_22', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_23', 'Next Token Predicted by Model': ' Africa'}, {'Layer name': 'Layer_24', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_25', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_26', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_27', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_28', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_29', 'Next Token Predicted by Model': ' Antar'}, {'Layer name': 'Layer_30', 'Next Token Predicted by Model': ' Antar'}], 'table_res': 'LLMs can be used to explain their own hidden layer representations through a framework called Patchscope. Predicted Next Tokens by Layer:\\nLayer 25:  Antar\\nLayer 26:  Antar\\nLayer 27:  Antar\\nLayer 28:  Antar\\nLayer 29:  Antar\\nLayer 30:  Antar'}], 'result_des': ''}\n"
     ]
    }
   ],
   "source": [
    "from diagnose import diagnose\n",
    "from dataset_process import knowns, ZsRE, counterfact, pararel, PEP3k, TwentyQ\n",
    "from models import llama, gptj, gpt2, qwen, internlm, baichuan\n",
    "from methods import patchscopes\n",
    "import methods\n",
    "dataset = knowns.KnownsDataset(loc=knowns.default_loc)\n",
    "sample = knowns.get_processed_kvs(dataset[0], patchscopes.requires_input_keys)\n",
    "result = diagnose.diagnosing(sample=sample, model_name_or_path=internlm, method=patchscopes.name)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
