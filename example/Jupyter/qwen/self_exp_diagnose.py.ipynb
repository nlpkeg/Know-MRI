{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44dabbc4-83ab-4165-b16d-1ebbf2b8eb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.Qwen-1_8B.modeling_qwen:The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "WARNING:transformers_modules.Qwen-1_8B.modeling_qwen:Try importing flash-attention for faster inference...\n",
      "WARNING:transformers_modules.Qwen-1_8B.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "WARNING:transformers_modules.Qwen-1_8B.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "WARNING:transformers_modules.Qwen-1_8B.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'kv_template' has no attribute 'dataset_info'\n",
      "Loaded dataset with 1209 elements\n",
      "loading model: /root/autodl-tmp/Qwen-1_8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005126476287841797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43b8d5077c848348ac98d8b810727fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'QWenLMHeadModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Qwen-1_8B model loaded\n",
      "{'origin_data': [\"Assess the correctness of the statement: 'Vinson Massif is located in the continent of Antarctica' Answer True if correct, False otherwise. Then, provide the most critical 4 words or phrases that influence your assessment.\"], 'image': '', 'table': [{'table_name': 'Self-explanation of LLM', 'table_list': [{'Input prompt': \"Assess the correctness of the statement: 'Vinson Massif is located in the continent of Antarctica' Answer True if correct, False otherwise. Then, provide the most critical 4 words or phrases that influence your assessment.\", 'answer': ' The task at hand involves assessing whether Vinson Massif is indeed a part of the continent of Antarctica.\\n\\nTo assess this claim, we need to gather information about both Vinson Massif and Antarctica. \\n\\nStep 1:\\nFirstly, I will search for reliable sources on the internet where I can find accurate data regarding these two entities.\\n \\nStep 2:\\nOnce I have gathered all relevant facts from credible sources, I\\'ll compare them with what\\'s known as \"consensus\" - which means there are multiple reputable sources'}], 'tabel_des': \"the most critical top_k words or phrases that influences LLM's assessment\", 'tabel_res': ''}], 'result_des': ''}\n"
     ]
    }
   ],
   "source": [
    "from diagnose import diagnose\n",
    "from dataset_process import knowns, ZsRE, counterfact, pararel, PEP3k, TwentyQ\n",
    "from models import llama, gptj, gpt2, qwen, t5, chatglm2, internlm, baichuan\n",
    "from methods import Selfexplanations\n",
    "\n",
    "dataset = knowns.KnownsDataset(loc=knowns.default_loc)\n",
    "sample = knowns.get_processed_kvs(dataset[0], Selfexplanations.requires_input_keys)\n",
    "\n",
    "result = diagnose.diagnosing(sample=sample, model_name_or_path=qwen, method=Selfexplanations.name)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
